all.Hazardous.All["Performance"] = c("Accuracy","MacroSensitivity","MacroSpecificity","MacroPrecision","MacroRecall","MacroF1","AUC","OptimalCutOff")
rm(mx)
#DT
dt.model.Hazardous <- rpart(Hazardous.int ~ Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,#Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,
data=asteroids_split$train, method="class", cp= 0.001) #. all var
dt.Hazardous.pred <- predict(dt.model.Hazardous, asteroids_split$test, type = "class")
dt.Hazardous.pred.prob <- predict(dt.model.Hazardous, asteroids_split$test, probability=TRUE)
dt.Hazardous.confusion_matrix_true = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="TRUE", mode = "prec_recall")
dt.Hazardous.confusion_matrix_false = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="FALSE", mode = "prec_recall")
setwd("~/Github/AsteroidsClassification")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(plyr)
library(gridExtra)
library(neuralnet)
library(e1071)
library(caret)
library(tidyverse)
library(ROCR)
library(gdata)
library(devtools)
opt.cut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]], specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.optcut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]],
specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.BIN.RNN <- function(model,testset,testLabels,classLabel){
ROCFun.pred = predict(model, testset,  probability=TRUE)
ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classLabel)
ROCFun.pred.to.roc = ROCFun.pred.prob[,ROCFun.pred.classindex]
ROCFun.pred.rocr = predictions(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
}
ROCFunction.BIN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI.RNN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
load("DATA_asteroids_dataset_split_0.7.RData")
asteroids_split$train$Hazardous.int = as.factor(asteroids_split$train$Hazardous)
asteroids_split$test$Hazardous.int = as.factor(asteroids_split$test$Hazardous)
#Hazardous
all.Hazardous <- list()
mx = matrix(NA, nrow = 3)
all.Hazardous$Accuracy = data.frame(mx)
all.Hazardous$MacroSensitivity <- data.frame(mx)
all.Hazardous$MacroSpecificity <- data.frame(mx)
all.Hazardous$MacroPrecision <- data.frame(mx)
all.Hazardous$MacroRecall <- data.frame(mx)
all.Hazardous$MacroF1 <- data.frame(mx)
all.Hazardous$AUC <- data.frame(mx)
all.Hazardous$CutOffOpt <- data.frame(mx)
all.Hazardous_ROC.x <- matrix()
all.Hazardous_ROC.y <- matrix()
all.Hazardous_ROC.name <- c(NA)
mx = matrix(NA, nrow = 8)
all.Hazardous.All <- data.frame(mx)
all.Hazardous.All["Performance"] = c("Accuracy","MacroSensitivity","MacroSpecificity","MacroPrecision","MacroRecall","MacroF1","AUC","OptimalCutOff")
rm(mx)
dt.model.Hazardous <- rpart(Hazardous.int ~ Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,#Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,
data=asteroids_split$train, method="class", cp= 0.001) #. all var
dt.Hazardous.pred <- predict(dt.model.Hazardous, asteroids_split$test, type = "class")
dt.Hazardous.pred.prob <- predict(dt.model.Hazardous, asteroids_split$test, probability=TRUE)
dt.Hazardous.confusion_matrix_true = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="TRUE", mode = "prec_recall")
dt.Hazardous.confusion_matrix_false = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="FALSE", mode = "prec_recall")
sens_true = dt.Hazardous.confusion_matrix_true$byClass["Sensitivity"]
spec_true= dt.Hazardous.confusion_matrix_true$byClass["Specificity"]
prec_true= dt.Hazardous.confusion_matrix_true$byClass["Precision"]
recal_true= dt.Hazardous.confusion_matrix_true$byClass["Recall"]
f1_true= dt.Hazardous.confusion_matrix_true$byClass["F1"]
sens_false =  dt.Hazardous.confusion_matrix_false$byClass["Sensitivity"]
spec_false =  dt.Hazardous.confusion_matrix_false$byClass["Specificity"]
prec_false =  dt.Hazardous.confusion_matrix_false$byClass["Precision"]
recal_false = dt.Hazardous.confusion_matrix_false$byClass["Recall"]
f1_false =    dt.Hazardous.confusion_matrix_false$byClass["F1"]
Accuracy = dt.Hazardous.confusion_matrix_true$overall['accuracy']
MacroSensitivity = (0.5 * sens_true) + (0.5 * sens_false)
MacroSpecificity = (0.5 * spec_true) + (0.5 * spec_false)
MacroPrecision = (0.5 * prec_true) + (0.5 * prec_false)
MacroRecall = (0.5 * recal_true) + (0.5 * recal_false)
MacroF1 = (0.5 * f1_true) + (0.5 * f1_false)
ROC = ROCFunction.BIN(dt.Hazardous.pred.prob,as.factor(asteroids_split$test$Hazardous),"TRUE")
dt.Hazardous.pred.prob
dt.Hazardous.pred.prob$'FALSE'
View(dt.Hazardous.pred.prob)
dt.Hazardous.pred.probasteroids_split$test
asteroids_split$test
dt.Hazardous.pred
setwd("~/Github/AsteroidsClassification")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(plyr)
library(gridExtra)
library(neuralnet)
library(e1071)
library(caret)
library(tidyverse)
library(ROCR)
library(gdata)
library(devtools)
opt.cut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]], specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.optcut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]],
specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.BIN.RNN <- function(model,testset,testLabels,classLabel){
ROCFun.pred = predict(model, testset,  probability=TRUE)
ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classLabel)
ROCFun.pred.to.roc = ROCFun.pred.prob[,ROCFun.pred.classindex]
ROCFun.pred.rocr = predictions(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
}
ROCFunction.BIN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI.RNN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
load("DATA_asteroids_dataset_split_0.7.RData")
asteroids_split$train$Hazardous.int = as.factor(asteroids_split$train$Hazardous)
asteroids_split$test$Hazardous.int = as.factor(asteroids_split$test$Hazardous)
#Hazardous
all.Hazardous <- list()
mx = matrix(NA, nrow = 3)
all.Hazardous$Accuracy = data.frame(mx)
all.Hazardous$MacroSensitivity <- data.frame(mx)
all.Hazardous$MacroSpecificity <- data.frame(mx)
all.Hazardous$MacroPrecision <- data.frame(mx)
all.Hazardous$MacroRecall <- data.frame(mx)
all.Hazardous$MacroF1 <- data.frame(mx)
all.Hazardous$AUC <- data.frame(mx)
all.Hazardous$CutOffOpt <- data.frame(mx)
all.Hazardous_ROC.x <- matrix()
all.Hazardous_ROC.y <- matrix()
all.Hazardous_ROC.name <- c(NA)
mx = matrix(NA, nrow = 8)
all.Hazardous.All <- data.frame(mx)
all.Hazardous.All["Performance"] = c("Accuracy","MacroSensitivity","MacroSpecificity","MacroPrecision","MacroRecall","MacroF1","AUC","OptimalCutOff")
rm(mx)
#DT
dt.model.Hazardous <- rpart(Hazardous.int ~ Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,#Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,
data=asteroids_split$train, method="class", cp= 0.001) #. all var
dt.Hazardous.pred <- predict(dt.model.Hazardous, asteroids_split$test, type = "class")
dt.Hazardous.pred.prob <- predict(dt.model.Hazardous, asteroids_split$test, probability=TRUE)
dt.Hazardous.pred
dt.Hazardous.pred.prob
dt.Hazardous.pred.prob[,2]
dt.Hazardous.pred.prob[,"true"]
dt.Hazardous.pred.prob[,"TRUE"]
as.vector(dt.Hazardous.pred.prob[,"TRUE"])
ROCFunction.MULTI.RNN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = as.vector(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
dt.model.Hazardous <- rpart(Hazardous.int ~ Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,#Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,
data=asteroids_split$train, method="class", cp= 0.001) #. all var
dt.Hazardous.pred <- predict(dt.model.Hazardous, asteroids_split$test, type = "class")
dt.Hazardous.pred.prob <- predict(dt.model.Hazardous, asteroids_split$test, probability=TRUE)
dt.Hazardous.confusion_matrix_true = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="TRUE", mode = "prec_recall")
dt.Hazardous.confusion_matrix_false = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="FALSE", mode = "prec_recall")
sens_true = dt.Hazardous.confusion_matrix_true$byClass["Sensitivity"]
spec_true= dt.Hazardous.confusion_matrix_true$byClass["Specificity"]
prec_true= dt.Hazardous.confusion_matrix_true$byClass["Precision"]
recal_true= dt.Hazardous.confusion_matrix_true$byClass["Recall"]
f1_true= dt.Hazardous.confusion_matrix_true$byClass["F1"]
sens_false =  dt.Hazardous.confusion_matrix_false$byClass["Sensitivity"]
spec_false =  dt.Hazardous.confusion_matrix_false$byClass["Specificity"]
prec_false =  dt.Hazardous.confusion_matrix_false$byClass["Precision"]
recal_false = dt.Hazardous.confusion_matrix_false$byClass["Recall"]
f1_false =    dt.Hazardous.confusion_matrix_false$byClass["F1"]
Accuracy = dt.Hazardous.confusion_matrix_true$overall['accuracy']
MacroSensitivity = (0.5 * sens_true) + (0.5 * sens_false)
MacroSpecificity = (0.5 * spec_true) + (0.5 * spec_false)
MacroPrecision = (0.5 * prec_true) + (0.5 * prec_false)
MacroRecall = (0.5 * recal_true) + (0.5 * recal_false)
MacroF1 = (0.5 * f1_true) + (0.5 * f1_false)
ROC = ROCFunction.BIN(dt.Hazardous.pred.prob,as.factor(asteroids_split$test$Hazardous),"TRUE")
ROCFun.pred.prob = dt.Hazardous.pred.prob
testLabels = as.factor(asteroids_split$test$Hazardous)
classInLabel= "TRUE"
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], recursive = TRUE, use.names=FALSE)
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels)
ROCFun.pred.to.roc = as.vector(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels)
ROCFun.pred.to.roc
testLabels
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, as.factor(testLabels) )
ROCFun.pred.to.roc = as.list(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, as.factor(testLabels) )
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.pred.prob = dt.Hazardous.pred.prob
testLabels = as.factor(asteroids_split$test$Hazardous)
classInLabel= "TRUE"
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = as.list(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.pred.to.roc
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.pred.to.roc
asteroids_split
asteroids_split
ROCFun.pred.to.roc = ROCFun.pred.to.roc %>% `rownames<-`( NULL )
ROCFun.pred.to.roc
dt.Hazardous.pred.prob = dt.Hazardous.pred.prob %>% `rownames<-`( NULL )
dt.Hazardous.pred.prob
setwd("~/Github/AsteroidsClassification")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(plyr)
library(gridExtra)
library(neuralnet)
library(e1071)
library(caret)
library(tidyverse)
library(ROCR)
library(gdata)
library(devtools)
opt.cut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]], specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.optcut = function(perf, pred){
cut.ind = mapply(FUN=function(x, y, p){
d = (x - 0)^2 + (y-1)^2
ind = which(d == min(d))
c(sensitivity = y[[ind]],
specificity = 1-x[[ind]],
cutoff = p[[ind]])
}, perf@x.values, perf@y.values, pred@cutoffs)
}
ROCFunction.BIN.RNN <- function(model,testset,testLabels,classLabel){
ROCFun.pred = predict(model, testset,  probability=TRUE)
ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classLabel)
ROCFun.pred.to.roc = ROCFun.pred.prob[,ROCFun.pred.classindex]
ROCFun.pred.rocr = predictions(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
}
ROCFunction.BIN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex], use.names=FALSE)
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI.RNN <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = as.vector(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
ROCFunction.MULTI <- function(ROCFun.pred.prob, testLabels, classInLabel){
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = as.vector(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.testClass = testLabels == classInLabel
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, ROCFun.testClass)
ROCFun.perf.rocr = performance(ROCFun.pred.rocr, measure = "auc", x.measure = "cutoff")
ROCFun.perf.tpr.rocr = performance(ROCFun.pred.rocr, "tpr","fpr")
ROCFun.perf.optcut = ROCFunction.optcut(ROCFun.perf.tpr.rocr, ROCFun.pred.rocr)
ROCFun.perf.optcut = ROCFun.perf.optcut[[3]]
return(c(
x.name = ROCFun.perf.tpr.rocr@x.name, x.value = ROCFun.perf.tpr.rocr@x.values,
y.name = ROCFun.perf.tpr.rocr@y.name, y.value = ROCFun.perf.tpr.rocr@y.values,
auc = ROCFun.perf.rocr@y.values, optcut = ROCFun.perf.optcut))
}
load("DATA_asteroids_dataset_split_0.7.RData")
asteroids_split$train$Hazardous.int = as.factor(asteroids_split$train$Hazardous)
asteroids_split$test$Hazardous.int = as.factor(asteroids_split$test$Hazardous)
#Hazardous
all.Hazardous <- list()
mx = matrix(NA, nrow = 3)
all.Hazardous$Accuracy = data.frame(mx)
all.Hazardous$MacroSensitivity <- data.frame(mx)
all.Hazardous$MacroSpecificity <- data.frame(mx)
all.Hazardous$MacroPrecision <- data.frame(mx)
all.Hazardous$MacroRecall <- data.frame(mx)
all.Hazardous$MacroF1 <- data.frame(mx)
all.Hazardous$AUC <- data.frame(mx)
all.Hazardous$CutOffOpt <- data.frame(mx)
all.Hazardous_ROC.x <- matrix()
all.Hazardous_ROC.y <- matrix()
all.Hazardous_ROC.name <- c(NA)
mx = matrix(NA, nrow = 8)
all.Hazardous.All <- data.frame(mx)
all.Hazardous.All["Performance"] = c("Accuracy","MacroSensitivity","MacroSpecificity","MacroPrecision","MacroRecall","MacroF1","AUC","OptimalCutOff")
rm(mx)
dt.model.Hazardous <- rpart(Hazardous.int ~ Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,#Orbit.Axis..AU. + Orbit.Eccentricity + Orbit.Inclination..deg. + Perihelion.Argument..deg. + Node.Longitude..deg. + Mean.Anomoly..deg. + Perihelion.Distance..AU. + Aphelion.Distance..AU. + Orbital.Period..yr. + Minimum.Orbit.Intersection.Distance..AU. + Asteroid.Magnitude,
data=asteroids_split$train, method="class", cp= 0.001) #. all var
dt.Hazardous.pred <- predict(dt.model.Hazardous, asteroids_split$test, type = "class")
dt.Hazardous.pred.prob <- predict(dt.model.Hazardous, asteroids_split$test, probability=TRUE)
dt.Hazardous.pred.prob = dt.Hazardous.pred.prob %>% `rownames<-`( NULL )
dt.Hazardous.confusion_matrix_true = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="TRUE", mode = "prec_recall")
dt.Hazardous.confusion_matrix_false = confusionMatrix(
dt.Hazardous.pred, asteroids_split$test$Hazardous.int,
positive="FALSE", mode = "prec_recall")
sens_true = dt.Hazardous.confusion_matrix_true$byClass["Sensitivity"]
spec_true= dt.Hazardous.confusion_matrix_true$byClass["Specificity"]
prec_true= dt.Hazardous.confusion_matrix_true$byClass["Precision"]
recal_true= dt.Hazardous.confusion_matrix_true$byClass["Recall"]
f1_true= dt.Hazardous.confusion_matrix_true$byClass["F1"]
sens_false =  dt.Hazardous.confusion_matrix_false$byClass["Sensitivity"]
spec_false =  dt.Hazardous.confusion_matrix_false$byClass["Specificity"]
prec_false =  dt.Hazardous.confusion_matrix_false$byClass["Precision"]
recal_false = dt.Hazardous.confusion_matrix_false$byClass["Recall"]
f1_false =    dt.Hazardous.confusion_matrix_false$byClass["F1"]
Accuracy = dt.Hazardous.confusion_matrix_true$overall['accuracy']
MacroSensitivity = (0.5 * sens_true) + (0.5 * sens_false)
MacroSpecificity = (0.5 * spec_true) + (0.5 * spec_false)
MacroPrecision = (0.5 * prec_true) + (0.5 * prec_false)
MacroRecall = (0.5 * recal_true) + (0.5 * recal_false)
MacroF1 = (0.5 * f1_true) + (0.5 * f1_false)
ROC = ROCFunction.BIN(dt.Hazardous.pred.prob,as.factor(asteroids_split$test$Hazardous),"TRUE")
dt.Hazardous.pred.prob
ROCFun.pred.prob = dt.Hazardous.pred.prob
testLabels = as.factor(asteroids_split$test$Hazardous)
classInLabel= "TRUE"
#ROCFun.pred = predict(model, testset,  probability=TRUE)
#ROCFun.pred.prob = attr(ROCFun.pred, "probabilities")
ROCFun.pred.class = colnames(ROCFun.pred.prob)
ROCFun.pred.classindex = which(ROCFun.pred.class == classInLabel)
ROCFun.pred.to.roc = unlist(ROCFun.pred.prob[,ROCFun.pred.classindex])
ROCFun.pred.to.roc
ROCFun.pred.rocr = prediction(ROCFun.pred.to.roc, testLabels )
testLabels
ROCFun.pred.to.roc
ROCFun.pred.rocr
prediction(ROCFun.pred.to.roc, testLabels )
